{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aboriginal-buffer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --train_root TRAIN_ROOT\n",
      "ipykernel_launcher.py: error: the following arguments are required: --train_root\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "from dataloader import APDDataset\n",
    "    \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train_root', type=str, required=True, default='./data/train')\n",
    "args = parser.parse_args()\n",
    "    \n",
    "\n",
    "def main():\n",
    "    train_root = args.train_root\n",
    "    train_set = APDDataset(root='./data/train',transform=train_transform)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_set, batch_size=16, shuffle=True, num_workers=1)\n",
    "    \n",
    "    '''\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        epoch_time_start = time.time()\n",
    "\n",
    "        #flag_validate_apd = (epoch + 1) % lfw_validation_epoch_interval == 0 or (epoch + 1) % epochs == 0\n",
    "        flag_validate_apd = True\n",
    "        train_loss_sum = 0\n",
    "        validation_loss_sum = 0\n",
    "\n",
    "        # Training the model\n",
    "        model.train()\n",
    "        #metric_fc.train()\n",
    "        learning_rate_scheduler.step()\n",
    "        #progress_bar = enumerate(tqdm(train_dataloader))\n",
    "        progress_bar = enumerate(train_dataloader)\n",
    "        \n",
    "        for batch_index, (data, labels) in progress_bar:\n",
    "            \n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "            print(data.shape)\n",
    "            #print('label', labels)\n",
    "\n",
    "            # Forward pass\n",
    "            #if flag_train_multi_gpu:\n",
    "            #    embedding, logits = model.module.forward_training(data)\n",
    "            #else:\n",
    "            #    embedding, logits = model.forward_training(data)\n",
    "\n",
    "            feature = model(data) ### embedding size \n",
    "            ### use ArcMargin to get output\n",
    "            output = metric_fc(feature, labels)\n",
    "            ### use CrossEntropy to calculate loss between output and label\n",
    "            # Calculate losses\n",
    "            cross_entropy_loss = criterion_crossentropy(output.cuda(), labels.cuda())\n",
    "\n",
    "\n",
    "            #center_loss = criterion_centerloss(embedding, labels)\n",
    "            #loss = (center_loss * center_loss_weight) + cross_entropy_loss\n",
    "            loss = cross_entropy_loss\n",
    "            logging.info(\"epoch:{}/{} batch_idx:{}/{} loss:{}\".format(epoch, end_epoch, batch_index, BATCH_NUM, loss))\n",
    "\n",
    "            # Backward pass\n",
    "            #optimizer_metric.zero_grad()\n",
    "            optimizer_model.zero_grad()\n",
    "            loss.backward()\n",
    "            #optimizer_metric.step()\n",
    "            optimizer_model.step()\n",
    "\n",
    "            # Remove center_loss_weight impact on the learning of center vectors\n",
    "            #for param in criterion_centerloss.parameters():\n",
    "            #    param.grad.data *= (1. / center_loss_weight)\n",
    "\n",
    "            # Update training loss sum\n",
    "            train_loss_sum += loss.item()*data.size(0)\n",
    "            #if batch_index == 20:\n",
    "            #    break\n",
    "\n",
    "        # Calculate average losses in epoch\n",
    "        avg_train_loss = train_loss_sum / len(train_dataloader.dataset)\n",
    "        \"\"\"\n",
    "        avg_validation_loss = validation_loss_sum / len(validation_dataloader.dataset)\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate training performance statistics in epoch\n",
    "        #classification_accuracy = correct * 100. / total\n",
    "        #classification_error = 100. - classification_accuracy\n",
    "\n",
    "        epoch_time_end = time.time()\n",
    "\n",
    "        print('Epoch {}:\\t Average Training Loss: {:.4f}\\t'.format(epoch+1, avg_train_loss))\n",
    "    '''\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-census",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
